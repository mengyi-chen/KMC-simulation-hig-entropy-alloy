"""
Step 2: Process structures and save for Cluster Expansion Training
SIMPLE SEQUENTIAL VERSION

This script computes correlation vectors using ClusterExpansionProcessor.
Key approach:
  - Uses subspace.structure_site_mapping() to get the correct site mapping
  - Uses processor.allowed_species[i].index(spec) to get the correct occupancy code

Usage:
    python 2_process_save_wrangler_simple.py
"""

import numpy as np
import json
from pathlib import Path
from tqdm import tqdm
import os
import time
import pickle

from pymatgen.core import Structure
from smol.cofe import ClusterSubspace
from smol.moca import ClusterExpansionProcessor


# Atomic number to symbol mapping (from 0_extract_local_envs_parallel.py)
ATOMIC_NUMBER_TO_SYMBOL = {
    3: 'Li',   # cation vacancy placeholder
    9: 'F',    # oxygen vacancy placeholder
    8: 'O',
    13: 'Al',
    24: 'Cr',
    26: 'Fe',
    27: 'Co',
    28: 'Ni',
    29: 'Cu'
}


def load_structure_from_npz(path):
    """Load structure from NPZ file."""
    data = np.load(path, allow_pickle=True)

    lattice = data['cell']
    coords = data['positions']
    species = data['symbols']

    return Structure(lattice, species, coords, coords_are_cartesian=True)


def create_primitive_structure(ref_path='./POSCAR.POSCAR.vasp'):
    """Create the primitive structure for the HEA spinel oxide."""
    unit = Structure.from_file(ref_path)
    print("Using CONVENTIONAL cubic cell")
    prim = unit.copy()
    prim.replace_species({
        'Fe': {'Cu': 1/7, 'Ni': 1/7, 'Fe': 1/7, 'Al': 1/7,'Cr': 1/7, 'Co': 1/7, 'Li': 1/7},
        'O': {'O': 1/2, 'F': 1/2}
    })

    return prim


def load_energies(energy_file):
    """Load MACE energies from JSON file.

    Returns:
        energy_map: dict mapping structure_idx (int) -> energy
    """
    with open(energy_file, 'r') as f:
        energy_data = json.load(f)

    energy_map = {}
    for entry in energy_data:
        structure_idx = entry['structure_idx']
        energy = entry.get('energy')
        if energy is not None:
            energy_map[structure_idx] = energy

    return energy_map


def build_species_to_code(processor):
    """
    Build a unified species -> code mapping from processor.allowed_species.

    Since different sublattices (metal vs anion) have independent encodings,
    the same code can map to different species (e.g., Li=0, O=0).
    This is fine as long as there's no conflict within the same sublattice.

    Args:
        processor: ClusterExpansionProcessor instance

    Returns:
        species_to_code: dict mapping species_str -> code
    """
    species_to_code = {}

    for site_idx, allowed_species in enumerate(processor.allowed_species):
        for code, sp in enumerate(allowed_species):
            sp_str = str(sp)
            if sp_str in species_to_code:
                if species_to_code[sp_str] != code:
                    raise RuntimeError(f"Conflict: {sp_str} has code {species_to_code[sp_str]} but site {site_idx} wants code {code}")
            else:
                species_to_code[sp_str] = code

    print(f"✅ Species to code mapping: {species_to_code}")
    return species_to_code


def build_atomic_number_to_code(species_to_code):
    """
    Build atomic_number -> code mapping from species_to_code.

    Args:
        species_to_code: dict mapping species_str -> code

    Returns:
        atomic_number_to_code: dict mapping atomic_number (int) -> code
    """
    # Reverse mapping: symbol -> atomic number
    symbol_to_atomic_number = {v: k for k, v in ATOMIC_NUMBER_TO_SYMBOL.items()}

    atomic_number_to_code = {}
    for species_str, code in species_to_code.items():
        if species_str in symbol_to_atomic_number:
            atomic_num = symbol_to_atomic_number[species_str]
            atomic_number_to_code[atomic_num] = code

    print(f"✅ Atomic number to code mapping: {atomic_number_to_code}")
    return atomic_number_to_code


def atomic_numbers_to_symbols(atomic_numbers):
    """
    Convert atomic numbers array to symbols array.

    Args:
        atomic_numbers: numpy array of atomic numbers (e.g., [3, 8, ...])

    Returns:
        symbols: numpy array of element symbols (e.g., ['Li', 'O', ...])
    """
    return np.array([ATOMIC_NUMBER_TO_SYMBOL[z] for z in atomic_numbers])


def symbols_to_occupancy(symbols, struct_idx_for_proc_site, species_to_code):
    """
    Convert symbols array to occupancy array.

    Args:
        symbols: numpy array of element symbols (e.g., ['Li', 'O', ...])
        struct_idx_for_proc_site: array mapping processor site index -> structure atom index
        species_to_code: dict mapping species string -> occupancy code

    Returns:
        occupancy: numpy array of occupancy codes
    """
    reordered_symbols = symbols[struct_idx_for_proc_site]
    return np.array([species_to_code[s] for s in reordered_symbols], dtype=np.int32)


def build_site_mapping(subspace, sc_matrix, template_structure):
    """
    Build site mapping using subspace.structure_site_mapping.

    This is the authoritative way to map between structure atom indices
    and processor site indices.

    Args:
        subspace: ClusterSubspace instance
        sc_matrix: supercell matrix
        template_structure: a template Structure to establish the mapping

    Returns:
        site_mapping: list where site_mapping[i] is the processor site index
                      for structure atom i
    """
    print("\n[3c] Building site mapping using subspace.structure_site_mapping...")
    t0 = time.time()

    # Create the supercell from subspace structure
    supercell = subspace.structure.copy()
    supercell.make_supercell(sc_matrix)

    # Get the site mapping
    site_mapping = subspace.structure_site_mapping(supercell, template_structure)

    print(f"✓ Site mapping built: {len(site_mapping)} sites")
    print(f"  Time: {time.time()-t0:.2f} seconds")
    print(f"site_mapping: {site_mapping[:20]} ...")

    return site_mapping


def main():
    # =========================================================================
    # Configuration
    # =========================================================================
    local_struct_dir = Path('./local_structures')
    train_energy_file = Path('./mace_energies/train_energies_medium_omat_0.json')
    output_dir = Path('./ce_data')
    os.makedirs(str(output_dir), exist_ok=True)

    print("="*60)
    print("Processing TRAIN Structures for CE (SIMPLE SEQUENTIAL)")
    print("="*60)

    # =========================================================================
    # 1. Create ClusterSubspace
    # =========================================================================
    print("\n[1] Creating ClusterSubspace...")
    prim = create_primitive_structure()
    print(f"Primitive cell atoms: {len(prim)}")

    subspace = ClusterSubspace.from_cutoffs(
        prim,
        ltol=0.2,
        stol=0.3,
        angle_tol=5,
        cutoffs={2: 6.0, 3: 4.0},
        basis='indicator',
        supercell_size='volume'
    )

    print(f"Correlation functions: {subspace.num_corr_functions}")
    print(f"Orbits: {len(subspace.orbits)}")


    # =========================================================================
    # 2. Load TRAIN data and create Processor
    # =========================================================================
    print("\n[2] Loading TRAIN data and creating Processor...")
    t0 = time.time()

    # Load train atomic numbers file
    train_file = local_struct_dir / 'train_atomic_numbers.npz'
    train_data = np.load(train_file)
    all_atomic_numbers = train_data['atomic_numbers']  # Shape: (N_structures, N_atoms)
    template_positions = train_data['positions']       # Shape: (N_atoms, 3)
    template_cell = train_data['cell']                 # Shape: (3, 3)

    print(f"Loaded train data: {all_atomic_numbers.shape[0]} structures, {all_atomic_numbers.shape[1]} atoms each")

    # Load train metadata
    metadata_file = local_struct_dir / 'train_metadata.json'
    with open(metadata_file, 'r') as f:
        metadata = json.load(f)
    print(f"Loaded metadata for {len(metadata)} train structures")

    # Load template structure from saved template file
    template_files = sorted(local_struct_dir.glob('template_*.npz'))
    template_path = str(template_files[0])
    template_structure = load_structure_from_npz(template_path)
    print(f"Template: {template_path}")
    
    # sc_matrix = np.array([[3, 0, 0], [0, 3, 0], [0, 0, 3]])
    sc_matrix = subspace.scmatrix_from_structure(template_structure)
    print(f"Supercell matrix:\n{sc_matrix}")

    dummy_coefficients = np.zeros(subspace.num_corr_functions)
    processor = ClusterExpansionProcessor(subspace, sc_matrix, dummy_coefficients)

    print(f"Processor built in {time.time()-t0:.1f} seconds")
    print(f"Supercell size: {processor.size} sites")
    print(f"Actual Processor Size: {processor.size}")
    print(f"Processor Structure Atoms: {len(processor.structure)}")


    # =========================================================================
    # 3. Build site mapping and species encoding
    # =========================================================================
    print("\n[3] Building mappings...")

    # Build unified species -> code mapping
    species_to_code = build_species_to_code(processor)

    # Build site mapping using subspace.structure_site_mapping
    site_mapping = build_site_mapping(subspace, sc_matrix, template_structure)

    # Number of sites in the occupancy array
    n_sites = len(processor.allowed_species)
    print(f"Number of sites in occupancy: {n_sites}")

    # Precompute the reverse mapping: for each processor site, which npz atom index to use
    # site_mapping[struct_idx] = proc_site_idx, so we need the inverse
    struct_idx_for_proc_site = np.array([site_mapping.index(i) for i in range(n_sites)])
    
    # =========================================================================
    # 4. Verify occupancy mapping
    # =========================================================================

    # Verify the mapping is correct by comparing with occupancy_from_structure
    # Use the saved template files for verification
    print("\n[4] Verifying occupancy mapping (using template structures)...")

    for i, template_file in enumerate(template_files[:3]):
        test_data = np.load(str(template_file), allow_pickle=True)
        test_symbols = test_data['symbols']
        test_structure = load_structure_from_npz(str(template_file))

        # Compute occupancy using our manual approach
        manual_occu = symbols_to_occupancy(test_symbols, struct_idx_for_proc_site, species_to_code)

        # Get correct occupancy from processor
        correct_occu = processor.occupancy_from_structure(test_structure)

        if np.array_equal(manual_occu, correct_occu):
            print(f"  ✓ Test {i+1}: {template_file.name} - PASS")
        else:
            mismatch_count = np.sum(manual_occu != correct_occu)
            print(f"  ✗ Test {i+1}: {template_file.name} - FAIL ({mismatch_count} mismatches)")
            print(f"    Manual (first 20): {manual_occu[:20]}")
            print(f"    Correct (first 20): {correct_occu[:20]}")
            raise RuntimeError("Occupancy mapping verification failed!")

    # =========================================================================
    # 5. Load train energies
    # =========================================================================
    print("\n[5] Loading train energies...")
    energy_map = load_energies(train_energy_file)
    print(f"Loaded {len(energy_map)} train energies")

    # =========================================================================
    # 6. Process all train structures
    # =========================================================================
    print("\n[6] Processing train structures...")

    # Find structures with energies
    n_total_structures = all_atomic_numbers.shape[0]
    n_atoms_per_structure = all_atomic_numbers.shape[1]

    # Count structures with available energies
    valid_indices = [idx for idx in range(n_total_structures) if idx in energy_map]
    print(f"Train structures with energies: {len(valid_indices)} / {n_total_structures}")

    # Process
    results = []
    start_time = time.time()

    for struct_idx in tqdm(valid_indices, desc="Computing correlations"):
        # Get atomic numbers and convert to symbols
        atomic_numbers = all_atomic_numbers[struct_idx]
        symbols = atomic_numbers_to_symbols(atomic_numbers)

        # Compute occupancy and correlation vector
        occupancy = symbols_to_occupancy(symbols, struct_idx_for_proc_site, species_to_code)
        corr_vector = processor.compute_feature_vector(occupancy)

        # Get n_real_atoms from metadata
        n_real_atoms = metadata[struct_idx]['n_real_atoms']

        results.append({
            'structure_idx': struct_idx,
            'corr_vector': corr_vector,
            'energy': energy_map[struct_idx],
            'n_atoms': n_atoms_per_structure,
            'n_real_atoms': n_real_atoms
        })

    elapsed_time = time.time() - start_time
    print(f"\nProcessing completed in {elapsed_time:.1f} seconds")
    print(f"Average: {elapsed_time/len(valid_indices)*1000:.2f} ms per structure")

    # =========================================================================
    # 7. Build feature matrix
    # =========================================================================
    print("\n[7] Building feature matrix...")
    n_structures = len(results)
    n_corr = subspace.num_corr_functions

    feature_matrix = np.zeros((n_structures, n_corr))
    energies = np.zeros(n_structures)
    structure_indices = []
    n_atoms_list = []
    n_real_atoms_list = []

    for i, result in enumerate(results):
        feature_matrix[i] = result['corr_vector']
        energies[i] = result['energy']
        structure_indices.append(result['structure_idx'])
        n_atoms_list.append(result['n_atoms'])
        n_real_atoms_list.append(result['n_real_atoms'])

    print(f"Feature matrix shape: {feature_matrix.shape}")
    print(f"Feature matrix rank: {np.linalg.matrix_rank(feature_matrix)}")

    # =========================================================================
    # 8. Process TEST data (if available)
    # =========================================================================
    test_file = local_struct_dir / 'test_atomic_numbers.npz'
    test_energy_file = Path('./mace_energies/test_energies_medium_omat_0.json')

    if test_file.exists() and test_energy_file.exists():
        print("\n[8] Processing TEST data...")

        # Load test data
        test_data = np.load(test_file)
        test_atomic_numbers = test_data['atomic_numbers']

        # Load test metadata
        test_metadata_file = local_struct_dir / 'test_metadata.json'
        with open(test_metadata_file, 'r') as f:
            test_metadata = json.load(f)

        # Load test energies
        test_energy_map = load_energies(test_energy_file)
        print(f"Loaded {len(test_energy_map)} test energies")

        n_test_structures = test_atomic_numbers.shape[0]
        test_valid_indices = [idx for idx in range(n_test_structures) if idx in test_energy_map]
        print(f"Test structures with energies: {len(test_valid_indices)} / {n_test_structures}")

        # Process test structures
        test_results = []
        test_start_time = time.time()

        for struct_idx in tqdm(test_valid_indices, desc="Computing test correlations"):
            atomic_numbers = test_atomic_numbers[struct_idx]
            symbols = atomic_numbers_to_symbols(atomic_numbers)
            occupancy = symbols_to_occupancy(symbols, struct_idx_for_proc_site, species_to_code)
            corr_vector = processor.compute_feature_vector(occupancy)
            n_real_atoms = test_metadata[struct_idx]['n_real_atoms']

            test_results.append({
                'structure_idx': struct_idx,
                'corr_vector': corr_vector,
                'energy': test_energy_map[struct_idx],
                'n_atoms': n_atoms_per_structure,
                'n_real_atoms': n_real_atoms
            })

        test_elapsed = time.time() - test_start_time
        print(f"Test processing completed in {test_elapsed:.1f} seconds")

        # Build test feature matrix
        n_test = len(test_results)
        test_feature_matrix = np.zeros((n_test, n_corr))
        test_energies = np.zeros(n_test)
        test_structure_indices = []
        test_n_atoms_list = []
        test_n_real_atoms_list = []

        for i, result in enumerate(test_results):
            test_feature_matrix[i] = result['corr_vector']
            test_energies[i] = result['energy']
            test_structure_indices.append(result['structure_idx'])
            test_n_atoms_list.append(result['n_atoms'])
            test_n_real_atoms_list.append(result['n_real_atoms'])

        print(f"Test feature matrix shape: {test_feature_matrix.shape}")

        # Save test data
        np.savez_compressed(
            output_dir / 'test_ce_data.npz',
            feature_matrix=test_feature_matrix,
            energies=test_energies,
            structure_indices=np.array(test_structure_indices),
            n_atoms=np.array(test_n_atoms_list),
            n_real_atoms=np.array(test_n_real_atoms_list)
        )
        print(f"Saved test data to: {output_dir / 'test_ce_data.npz'}")
    else:
        print("\n[8] No TEST data found - skipping test processing")

    # =========================================================================
    # 9. Save TRAIN results
    # =========================================================================
    print("\n[9] Saving TRAIN results...")

    # Save feature matrix and energies
    np.savez_compressed(
        output_dir / 'ce_data.npz',
        feature_matrix=feature_matrix,
        energies=energies,
        structure_indices=np.array(structure_indices),
        n_atoms=np.array(n_atoms_list),
        n_real_atoms=np.array(n_real_atoms_list)
    )

    # Save subspace and processor
    with open(output_dir / 'subspace.pkl', 'wb') as f:
        pickle.dump(subspace, f)

    with open(output_dir / 'processor.pkl', 'wb') as f:
        pickle.dump(processor, f)

    # Save site mapping (struct_idx -> proc_site_idx)
    with open(output_dir / 'site_mapping.json', 'w') as f:
        json.dump(site_mapping, f)

    # Save reverse mapping (proc_site_idx -> struct_idx)
    with open(output_dir / 'struct_idx_for_proc_site.json', 'w') as f:
        json.dump(struct_idx_for_proc_site.tolist(), f)

    # Save species to code mapping
    with open(output_dir / 'species_to_code.json', 'w') as f:
        json.dump(species_to_code, f, indent=2)

    # Save stats
    stats = {
        'n_structures': n_structures,
        'n_corr_functions': n_corr,
        'n_orbits': len(subspace.orbits),
        'n_sites': processor.size,
        'energy_min': float(np.min(energies)),
        'energy_max': float(np.max(energies)),
        'energy_mean': float(np.mean(energies)),
        'energy_std': float(np.std(energies)),
        'cutoffs': {'pair': 6.0, 'triplet': 4.0},
        'basis': 'indicator',
        'processing_time_seconds': elapsed_time
    }

    with open(output_dir / 'stats.json', 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"\nSaved:")
    print(f"  - {output_dir / 'ce_data.npz'}")
    print(f"  - {output_dir / 'subspace.pkl'}")
    print(f"  - {output_dir / 'processor.pkl'}")
    print(f"  - {output_dir / 'site_mapping.json'}")
    print(f"  - {output_dir / 'struct_idx_for_proc_site.json'}")
    print(f"  - {output_dir / 'species_to_code.json'}")
    print(f"  - {output_dir / 'stats.json'}")

    print(f"\nStatistics:")
    print(f"  Structures: {stats['n_structures']}")
    print(f"  Correlation functions: {stats['n_corr_functions']}")
    print(f"  Sites per structure: {stats['n_sites']}")
    print(f"  Energy range: [{stats['energy_min']:.2f}, {stats['energy_max']:.2f}] eV")

    print("\n" + "="*60)
    print("Processing Complete!")
    print("="*60)


if __name__ == '__main__':
    main()
